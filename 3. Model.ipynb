{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attempt 1\n",
    "Initially, I set out to have a simple star schema with one fact table for Covid-19 cases and deaths over time, with a references to counties and states. This could have worked since I could have treated states as a special kind of county (states technically have a FIPS code as well, which is just the 2-digit state code followed by three zeroes). The county table would be a dimension table that is reference from the Covid-19 fact table.\n",
    "\n",
    "I decided against this for the following reasons:\n",
    "* I want counties to refer to the states they're in. If states are counties as well, it gets weird quickly because I'd need to have counties have a foreign key to... another county (which is just a state in a trenchcoat). This just opens us up to data corruption because we'll somehow end up with a county referring to something that isn't a state.\n",
    "* It's not intuitive. I wouldn't expect to see states in a table for counties.\n",
    "* Some of my data sources (weather, and optionally election and mobility) don't have data for states, so I'd have to awkwardly infer it somehow, or leave those columns empty. Again, this assumes that the end user understands that the county data contains \"special counties\", i.e. states.\n",
    "\n",
    "#### Attempt 2\n",
    "Alright, we'll have two dimension tables, then: One for counties, and one for states. So how does this affect the facts table? We'd now need each row to refer to both a county and a state, but only one of these would ever be set at a time, the other being \"None\", since we can't have a row that has data for both a state and a county. That's still weird, I don't want to have None-values without good reasons, and I don't want end users to have to remember this when dealing with my data.\n",
    "\n",
    "Additionally, some of my supplementary data sources are time series as well! Weather data is recorded by date, and so is mobility data. But these aren't available for states, and generating averages for states based on county data feels icky (e.g. California is a huge state, I'd expect the weather data to differ a lot between the North and South ends, an average wouldn't be very useful).  \n",
    "But wouldn't it be great to record those time series in the same fact table, since they also use the county and the date? So in addition to Covid-19 data, this time series fact table could contain data about the day-to-day weather (and, later on, resident mobility)!\n",
    "\n",
    "#### Attempt 3\n",
    "Since this data isn't avaiable for states, maybe let's split the facts table in two: One for a county time series, and one for a state time series. We can augment the county facts table with weather and mobility data, but leave the states only using Covid-19 data.\n",
    "\n",
    "Now, as an alternative, we could not create a facts table for states and instead generate aggregates for case and death numbers based on counties, since they refer to states. The downside would be that if we wanted to query case numbers and combine it with information about the state, we'd need to join the facts table with counties and states. This kind of query seems common enough to me that I wouldn't want to require users to constantly run groupBy-aggregates and a three-way join; instead, we can just bake that data into another facts table once and then access it directly.  \n",
    "I'm retaining the reference to the state from the county table. I'd expect the reference be used for grouping by state or selecting only counties in a specific state, so generally we wouldn't need to join with the state dimension table. I can however see some cases where it would be advantageous to join with the state table; for example, if we want to find counties that are doing better/worse in some metric than the state average (averaging grouped counties in a state won't work in this case, since counties have different populations, so statistics based on percentages of popuplation will be distorted by averaging indiscriminately).\n",
    "\n",
    "#### Conclusion\n",
    "All in all, this leaves us with the following schema:\n",
    "\n",
    "<img src=\"images/database_schema.png\">\n",
    "\n",
    "(This database schema visualisation was created using [SQLDBM](app.sqldbm.com))\n",
    "\n",
    "#### Columns\n",
    "County IDs are FIPS codes which are 5 digits, hence the character limit. State IDs are state name abbreviations which are always two letters. I hope that 50 characters are enough for county and state names, but I'll adjust this if I run into trouble later on.  \n",
    "The county and state dimension tables allow NULL values for some columns. These are the columns that I identified to have NaNs during exploratory analysis. I'd rather require end users make a check to skip those rows than to generate questionable replacement data and lead them to draw incorrect conclusions, especially if it is clearly telegraphed in the table configuration that these columns will have NULL-values.  \n",
    "Also note how the state dimension table has a lot fewer columns with NULL-values.\n",
    "\n",
    "The fact tables for county and state time series contain columns for Covid-19 case and death counts, both as a total and as a delta to the previous day. We could do with either of these and then calculate the other one based on it, but I predict that both versions will be in common use, so I'd rather build them in instead of repeatedly calculating them for each request later on.\n",
    "\n",
    "The date table uses a timestamp as a primary key, and at table population I will fill in the timestamp components instead of having to extract them constantly at execution time.\n",
    "\n",
    "#### Table creation\n",
    "The resulting table creation (for Redshift) can be found in the \"sql\" folder, in the \"create_tables.sql\" file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outline\n",
    "Pipeline:\n",
    "- Only use counties and states that are in the Covid and health data sets\n",
    "- Use spark, mostly rely on dataframes but sometimes use temporary database views for convenience\n",
    "- End result should be outputting parquet files\n",
    "\n",
    "County facts:\n",
    "- Start with the Covid-19 data set, use the timestamps to link to time, use FIPS to link to counties\n",
    "- Add total covid cases and deaths from the data set\n",
    "- Calculate delta based on current and previous columns during insert so we don't have to calculate the previous date and extract that data later\n",
    "- Augment with weather data based on the timestamp and county\n",
    "- Partition by county, then month\n",
    "\n",
    "State dim:\n",
    "- Create state dimension table from health data as a start\n",
    "- Calculate area and population density based on county dim table, group by state\n",
    "- Don't partition? There's only 50 of them anyway\n",
    "\n",
    "State facts:\n",
    "- Start Covid-19 data, use the state abbreviation as a link to states, timestamp to time\n",
    "- Calculate aggregate Covid-19 cases and deaths (both total and delta) for each day based on county facts table, group by state\n",
    "- Partition by state, then month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup\n",
    "I'm going to need Spark for this because I'll want to make use of some of its functionality, such as the ability to create temporary SQL views of my dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup import create_spark_session\n",
    "\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, from_unixtime\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from scripts.clean import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, just locally, later on maybe write this to S3 instead\n",
    "output_path = \"output/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Time dimension table\n",
    "- Create using pandas, start at 1st of January 2020, go in one-day intervals until 31st of December 2020 and create a timestamp for each day\n",
    "- Extract time components\n",
    "- Partition by month (better would be year, then month, but we're only doing 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date\n",
       "0 2020-01-01\n",
       "1 2020-01-02\n",
       "2 2020-01-03\n",
       "3 2020-01-04\n",
       "4 2020-01-05"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_df_pd = pd.DataFrame({'date':pd.date_range('2020-01-01', '2020-12-31')})\n",
    "time_df_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date    366\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_df_pd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|               date|\n",
      "+-------------------+\n",
      "|2020-01-01 00:00:00|\n",
      "|2020-01-02 00:00:00|\n",
      "|2020-01-03 00:00:00|\n",
      "|2020-01-04 00:00:00|\n",
      "|2020-01-05 00:00:00|\n",
      "|2020-01-06 00:00:00|\n",
      "|2020-01-07 00:00:00|\n",
      "|2020-01-08 00:00:00|\n",
      "|2020-01-09 00:00:00|\n",
      "|2020-01-10 00:00:00|\n",
      "|2020-01-11 00:00:00|\n",
      "|2020-01-12 00:00:00|\n",
      "|2020-01-13 00:00:00|\n",
      "|2020-01-14 00:00:00|\n",
      "|2020-01-15 00:00:00|\n",
      "|2020-01-16 00:00:00|\n",
      "|2020-01-17 00:00:00|\n",
      "|2020-01-18 00:00:00|\n",
      "|2020-01-19 00:00:00|\n",
      "|2020-01-20 00:00:00|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_df = spark.createDataFrame(time_df_pd)\n",
    "time_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark 3.0+ for some reason removed the ability to parse weekdays into integers, it only supports strings now.\n",
    "# Don't ask me why, I can't see how that's a good restriction to add.\n",
    "# We can fall back to the legacy time parser to restore the old behaviour.\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+----+-----+----+-------+\n",
      "|               date|day|week|month|year|weekday|\n",
      "+-------------------+---+----+-----+----+-------+\n",
      "|2020-01-01 00:00:00|  1|   1|    1|2020|      3|\n",
      "|2020-01-02 00:00:00|  2|   1|    1|2020|      4|\n",
      "|2020-01-03 00:00:00|  3|   1|    1|2020|      5|\n",
      "|2020-01-04 00:00:00|  4|   1|    1|2020|      6|\n",
      "|2020-01-05 00:00:00|  5|   1|    1|2020|      7|\n",
      "|2020-01-06 00:00:00|  6|   2|    1|2020|      1|\n",
      "|2020-01-07 00:00:00|  7|   2|    1|2020|      2|\n",
      "|2020-01-08 00:00:00|  8|   2|    1|2020|      3|\n",
      "|2020-01-09 00:00:00|  9|   2|    1|2020|      4|\n",
      "|2020-01-10 00:00:00| 10|   2|    1|2020|      5|\n",
      "|2020-01-11 00:00:00| 11|   2|    1|2020|      6|\n",
      "|2020-01-12 00:00:00| 12|   2|    1|2020|      7|\n",
      "|2020-01-13 00:00:00| 13|   3|    1|2020|      1|\n",
      "|2020-01-14 00:00:00| 14|   3|    1|2020|      2|\n",
      "|2020-01-15 00:00:00| 15|   3|    1|2020|      3|\n",
      "|2020-01-16 00:00:00| 16|   3|    1|2020|      4|\n",
      "|2020-01-17 00:00:00| 17|   3|    1|2020|      5|\n",
      "|2020-01-18 00:00:00| 18|   3|    1|2020|      6|\n",
      "|2020-01-19 00:00:00| 19|   3|    1|2020|      7|\n",
      "|2020-01-20 00:00:00| 20|   4|    1|2020|      1|\n",
      "+-------------------+---+----+-----+----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_df = time_df.withColumn('day', dayofmonth('date')) \\\n",
    "    .withColumn('week', weekofyear('date')) \\\n",
    "    .withColumn('month', month('date')) \\\n",
    "    .withColumn('year', year('date')) \\\n",
    "    .withColumn('weekday', date_format(col(\"date\"), \"u\"))\n",
    "time_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----+-----+----+-------+\n",
      "|      date|day|week|month|year|weekday|\n",
      "+----------+---+----+-----+----+-------+\n",
      "|2020-01-01|  1|   1|    1|2020|      3|\n",
      "|2020-01-02|  2|   1|    1|2020|      4|\n",
      "|2020-01-03|  3|   1|    1|2020|      5|\n",
      "|2020-01-04|  4|   1|    1|2020|      6|\n",
      "|2020-01-05|  5|   1|    1|2020|      7|\n",
      "|2020-01-06|  6|   2|    1|2020|      1|\n",
      "|2020-01-07|  7|   2|    1|2020|      2|\n",
      "|2020-01-08|  8|   2|    1|2020|      3|\n",
      "|2020-01-09|  9|   2|    1|2020|      4|\n",
      "|2020-01-10| 10|   2|    1|2020|      5|\n",
      "|2020-01-11| 11|   2|    1|2020|      6|\n",
      "|2020-01-12| 12|   2|    1|2020|      7|\n",
      "|2020-01-13| 13|   3|    1|2020|      1|\n",
      "|2020-01-14| 14|   3|    1|2020|      2|\n",
      "|2020-01-15| 15|   3|    1|2020|      3|\n",
      "|2020-01-16| 16|   3|    1|2020|      4|\n",
      "|2020-01-17| 17|   3|    1|2020|      5|\n",
      "|2020-01-18| 18|   3|    1|2020|      6|\n",
      "|2020-01-19| 19|   3|    1|2020|      7|\n",
      "|2020-01-20| 20|   4|    1|2020|      1|\n",
      "+----------+---+----+-----+----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Even though the original pandas dataframe used datetime, the spark dataframe reverted to timestamp.\n",
    "# I really don't need the time-of-day parts, so let's force this back to datetime.\n",
    "time_df = time_df.withColumn('date', time_df['date'].cast(DateType()))\n",
    "time_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df.write.partitionBy('month').mode('overwrite').parquet(output_path + \"time.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### County dimension table:\n",
    "- Create county dimension table from Covid-19 data as a start\n",
    "- Augment county table with health data\n",
    "- Augment with area and population density (using health data and area data)\n",
    "- Use state abbreviation as foreign key for state table (which is created afterwards)\n",
    "- Partition by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_cases_df = clean_covid_data(spark, \"data/covid_cases_US.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "branch-env",
   "language": "python",
   "name": "branch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
