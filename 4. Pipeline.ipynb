{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run pipelines to model the data\n",
    "Now that our data are cleaned, it's time to get them into a format that is more representative of the final relational database and more easily read by them.\n",
    "\n",
    "I'm using Spark for the data transformation, more precisely PySpark. It's a good tool for dealing with large amounts of data in parallel, and supports the final output format of my choice: Parquet.\n",
    "\n",
    "As discussed in the previous step, we'll need to create five tables in total. I've split out each table into its own notebook. In each notebook, we'll explore how to best combine our various data sources, do some preliminary data quality checks and fix up the data some more where needed, and finally output the results locally. Once we're happy with the pipeline, we'll extract the code into the etl.py file that will eventually contain the whole pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "branch-env",
   "language": "python",
   "name": "branch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
