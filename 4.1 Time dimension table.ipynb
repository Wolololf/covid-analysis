{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Create the time dimension table\n",
    "The time dimension table needs to cover every day in 2020, but we only need the date-aspect, we're not interested in the hours, minutes and seconds. We will extract the necessary components and populate the table accordingly.  \n",
    "We will partition the table by month (normally would do year, then month, but in this case the data is limited to one year)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup\n",
    "I'm going to need Spark for this because I'll want to make use of some of its functionality, such as the ability to create temporary SQL views of my dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup import create_spark_session\n",
    "\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and output paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from clean import *\n",
    "from etl import *\n",
    "\n",
    "# For now, just locally, later on maybe write this to S3 instead\n",
    "output_path = \"output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date\n",
       "0 2020-01-01\n",
       "1 2020-01-02\n",
       "2 2020-01-03\n",
       "3 2020-01-04\n",
       "4 2020-01-05"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_df_pd = pd.DataFrame({'date':pd.date_range('2020-01-01', '2020-12-31')})\n",
    "time_df_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date    366\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_df_pd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|               date|\n",
      "+-------------------+\n",
      "|2020-01-01 00:00:00|\n",
      "|2020-01-02 00:00:00|\n",
      "|2020-01-03 00:00:00|\n",
      "|2020-01-04 00:00:00|\n",
      "|2020-01-05 00:00:00|\n",
      "|2020-01-06 00:00:00|\n",
      "|2020-01-07 00:00:00|\n",
      "|2020-01-08 00:00:00|\n",
      "|2020-01-09 00:00:00|\n",
      "|2020-01-10 00:00:00|\n",
      "|2020-01-11 00:00:00|\n",
      "|2020-01-12 00:00:00|\n",
      "|2020-01-13 00:00:00|\n",
      "|2020-01-14 00:00:00|\n",
      "|2020-01-15 00:00:00|\n",
      "|2020-01-16 00:00:00|\n",
      "|2020-01-17 00:00:00|\n",
      "|2020-01-18 00:00:00|\n",
      "|2020-01-19 00:00:00|\n",
      "|2020-01-20 00:00:00|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_df = spark.createDataFrame(time_df_pd)\n",
    "time_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark 3.0+ for some reason removed the ability to parse weekdays into integers, it only supports strings now.\n",
    "# Don't ask me why, I can't see how that's a good restriction to add.\n",
    "# We can fall back to the legacy time parser to restore the old behaviour.\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+----+-----+----+-------+\n",
      "|               date|day|week|month|year|weekday|\n",
      "+-------------------+---+----+-----+----+-------+\n",
      "|2020-01-01 00:00:00|  1|   1|    1|2020|      3|\n",
      "|2020-01-02 00:00:00|  2|   1|    1|2020|      4|\n",
      "|2020-01-03 00:00:00|  3|   1|    1|2020|      5|\n",
      "|2020-01-04 00:00:00|  4|   1|    1|2020|      6|\n",
      "|2020-01-05 00:00:00|  5|   1|    1|2020|      7|\n",
      "|2020-01-06 00:00:00|  6|   2|    1|2020|      1|\n",
      "|2020-01-07 00:00:00|  7|   2|    1|2020|      2|\n",
      "|2020-01-08 00:00:00|  8|   2|    1|2020|      3|\n",
      "|2020-01-09 00:00:00|  9|   2|    1|2020|      4|\n",
      "|2020-01-10 00:00:00| 10|   2|    1|2020|      5|\n",
      "|2020-01-11 00:00:00| 11|   2|    1|2020|      6|\n",
      "|2020-01-12 00:00:00| 12|   2|    1|2020|      7|\n",
      "|2020-01-13 00:00:00| 13|   3|    1|2020|      1|\n",
      "|2020-01-14 00:00:00| 14|   3|    1|2020|      2|\n",
      "|2020-01-15 00:00:00| 15|   3|    1|2020|      3|\n",
      "|2020-01-16 00:00:00| 16|   3|    1|2020|      4|\n",
      "|2020-01-17 00:00:00| 17|   3|    1|2020|      5|\n",
      "|2020-01-18 00:00:00| 18|   3|    1|2020|      6|\n",
      "|2020-01-19 00:00:00| 19|   3|    1|2020|      7|\n",
      "|2020-01-20 00:00:00| 20|   4|    1|2020|      1|\n",
      "+-------------------+---+----+-----+----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_df = time_df.withColumn('day', dayofmonth('date')) \\\n",
    "    .withColumn('week', weekofyear('date')) \\\n",
    "    .withColumn('month', month('date')) \\\n",
    "    .withColumn('year', year('date')) \\\n",
    "    .withColumn('weekday', date_format(col(\"date\"), \"u\"))\n",
    "time_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----+-----+----+-------+\n",
      "|      date|day|week|month|year|weekday|\n",
      "+----------+---+----+-----+----+-------+\n",
      "|2020-01-01|  1|   1|    1|2020|      3|\n",
      "|2020-01-02|  2|   1|    1|2020|      4|\n",
      "|2020-01-03|  3|   1|    1|2020|      5|\n",
      "|2020-01-04|  4|   1|    1|2020|      6|\n",
      "|2020-01-05|  5|   1|    1|2020|      7|\n",
      "|2020-01-06|  6|   2|    1|2020|      1|\n",
      "|2020-01-07|  7|   2|    1|2020|      2|\n",
      "|2020-01-08|  8|   2|    1|2020|      3|\n",
      "|2020-01-09|  9|   2|    1|2020|      4|\n",
      "|2020-01-10| 10|   2|    1|2020|      5|\n",
      "|2020-01-11| 11|   2|    1|2020|      6|\n",
      "|2020-01-12| 12|   2|    1|2020|      7|\n",
      "|2020-01-13| 13|   3|    1|2020|      1|\n",
      "|2020-01-14| 14|   3|    1|2020|      2|\n",
      "|2020-01-15| 15|   3|    1|2020|      3|\n",
      "|2020-01-16| 16|   3|    1|2020|      4|\n",
      "|2020-01-17| 17|   3|    1|2020|      5|\n",
      "|2020-01-18| 18|   3|    1|2020|      6|\n",
      "|2020-01-19| 19|   3|    1|2020|      7|\n",
      "|2020-01-20| 20|   4|    1|2020|      1|\n",
      "+----------+---+----+-----+----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Even though the original pandas dataframe used datetime, the spark dataframe reverted to timestamp.\n",
    "# I really don't need the time-of-day parts, so let's force this back to datetime.\n",
    "time_df = time_df.withColumn('date', time_df['date'].cast(DateType()))\n",
    "time_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df.write.partitionBy('month').mode('overwrite').parquet(output_path + \"time.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
