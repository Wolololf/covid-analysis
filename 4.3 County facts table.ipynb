{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Create the county facts table\n",
    "- Start with covid case data, split out date columns into a table with county-date-case rows\n",
    "- Window with county partition and date sorting, calculate data by lagging one behind with 0 fill\n",
    "- Do the same for covid death data, then join\n",
    "- Same for all the weather data, keep joining\n",
    "- Write to parquet partitioned by day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup\n",
    "I'm going to need Spark for this because I'll want to make use of some of its functionality, such as the ability to create temporary SQL views of my dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup import create_spark_session\n",
    "\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and output paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from clean import *\n",
    "from etl import *\n",
    "\n",
    "# For now, just locally, later on maybe write this to S3 instead\n",
    "output_path = \"output/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first load in the cleaned Covid data and inspect the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started loading Covid-19 case data\n",
      "Finished loading Covid-19 case data\n"
     ]
    }
   ],
   "source": [
    "covid_cases_df = load_covid_case_data(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's extract all the dates from the Covid-19 data set. This is so we can avoid having to cast the column names to dates constantly, instead we can do it once and cache them here.  \n",
    "Apparently, I have to store them as unix timestamps; I tried keeping them as DateType, but when I later use them to construct the dataframe, it just gives me empty lists instead of datetimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1579651200, 1579737600, 1579824000, 1579910400, 1579996800]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unix_time = pd.Timestamp(\"1970-01-01\")\n",
    "second = pd.Timedelta('1s')\n",
    "\n",
    "date_list = [(pd.to_datetime(c) - unix_time) // second for c in covid_cases_df.columns[5:]]\n",
    "date_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need the FIPS codes and the time columns, let's extract these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fips', 'state', '1/22/20', '1/23/20', '1/24/20']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_data_columns = covid_cases_df.columns[5:]\n",
    "time_data_columns.insert(0, 'fips')\n",
    "time_data_columns.insert(1, 'state')\n",
    "\n",
    "time_data_columns[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each row in the dataframe, for each time column, we want to create one new row in a new dataframe where each row contains the FIPS code (to identify the county), the date of the column, and the case count for this county at that date.  \n",
    "We store these as a tuple in the list so that we can insert each tuple as a row into a dataframe later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1001, 'Alabama', 1579651200, 0),\n",
       " (1001, 'Alabama', 1579737600, 0),\n",
       " (1001, 'Alabama', 1579824000, 0),\n",
       " (1001, 'Alabama', 1579910400, 0),\n",
       " (1001, 'Alabama', 1579996800, 0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series = []\n",
    "\n",
    "def extract_county_case_data(row):\n",
    "    fips = time_data_columns[0]\n",
    "    state = time_data_columns[1]\n",
    "    for i in range(2, len(time_data_columns)):\n",
    "        time_series.append((row[fips], row[state], date_list[i - 2], row[time_data_columns[i]]))\n",
    "\n",
    "# I have no idea why the foreach doesn't work, I can only get it working if I collect the data\n",
    "#covid_cases_df.limit(5).foreach(extract_county_case_data)\n",
    "\n",
    "for row in covid_cases_df.collect():\n",
    "    extract_county_case_data(row)\n",
    "\n",
    "time_series[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got the values for the rows, now we just need to define the column names and build the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_columns = ['fips', 'state', 'timestamp', 'covid_case_total']\n",
    "\n",
    "county_cases_df = spark.createDataFrame(time_series, time_series_columns)\n",
    "county_cases_df.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could just keep the timestamps? I don't know if we need the datetimes since we'll fetch these from another table. I'll keep these in for now because it makes it easier for me to tell which dates I'm dealing with while debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_cases_df = county_cases_df.withColumn(\"date\", F.from_unixtime(\"timestamp\").cast(DateType()))\n",
    "county_cases_df.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only have the cumulative Covid-19 case counts, not the daily increase. Since we're most likely going to check the delta quite frequently, let's add that to the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window \\\n",
    "    .partitionBy(county_cases_df['fips']) \\\n",
    "    .orderBy(county_cases_df['timestamp'].asc())\n",
    "\n",
    "''' This is more long-form, I wrote this because I thought that lagged numbers were bleeding across partition boundaries, but that doesn't seem to be the case.\n",
    "county_cases_df = county_cases_df.withColumn('lag', F.lag(county_cases_df['covid_case_total']).over(windowSpec))\n",
    "\n",
    "county_cases_df = county_cases_df.withColumn('covid_case_delta', \\\n",
    "    F.when(county_cases_df['lag'].isNull(), 0) \\\n",
    "    .otherwise(county_cases_df['covid_case_total'] - county_cases_df['lag']))\n",
    "\n",
    "county_cases_df = county_cases_df.drop('lag')\n",
    "'''\n",
    "\n",
    "county_cases_df = county_cases_df.withColumn('covid_case_delta', \\\n",
    "    county_cases_df['covid_case_total'] - F.lag(county_cases_df['covid_case_total'], 1, 0).over(windowSpec))\n",
    "\n",
    "county_cases_df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_cases_df.where(county_cases_df['covid_case_delta'].isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so good, no null entries in the delta column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_cases_df.agg({'covid_case_delta': 'max'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_cases_df.agg({'covid_case_delta': 'min'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, what? Why do we have negative case counts here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_cases_df.where(county_cases_df['covid_case_delta'] < 0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_cases_df.where(county_cases_df['covid_case_delta'] < 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_cases_df.where(county_cases_df['covid_case_delta'] > 0).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be a fair few cases of this, but there aren't that many compared to the amount of actual increases, so it seems to work in most cases. It's also not just occurring once per FIPS code which might indicate an issue with my windowing logic.\n",
    "\n",
    "At a guess, this is due to overreporting some cases one day and then correcting them the next day. These are case numbers as well, so it's possible that someone received a false positive and got scrubbed from the case counts again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_cases_df.where(county_cases_df['covid_case_delta'] < -20).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_cases_df.where(county_cases_df['covid_case_delta'] < -20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so there are still several hundred cases where more than 20 negative cases were reported. Still, this doesn't seem to be a bug, but just something inherent to the data. I checked a few of these by hand in the source data, and the data on days right after the decrease looks fine again, it jumps back up to the previous value and beyond. This makes me think that at least some of these are mistakes in data entry.\n",
    "\n",
    "I'll attempt to fix up the broken deltas and totals by interpolating using the neighbouring rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_county_cases_df = county_cases_df.withColumn('lag', F.lag(county_cases_df['covid_case_total'], 1).over(windowSpec))\n",
    "adjusted_county_cases_df = adjusted_county_cases_df.withColumn('lead', F.lead(county_cases_df['covid_case_total'], 1).over(windowSpec))\n",
    "adjusted_county_cases_df = adjusted_county_cases_df.withColumn('next_delta', F.lead(county_cases_df['covid_case_delta'], 1).over(windowSpec))\n",
    "\n",
    "# Get rid of overreporting\n",
    "adjusted_county_cases_df = adjusted_county_cases_df.withColumn('covid_case_total', \\\n",
    "    F.when((adjusted_county_cases_df['next_delta'] >= 0) | adjusted_county_cases_df['lag'].isNull() | (adjusted_county_cases_df['lead'].isNull()), county_cases_df['covid_case_total']) \\\n",
    "    .otherwise(F.ceil((adjusted_county_cases_df['lead'] + adjusted_county_cases_df['lag']) / 2)))\n",
    "\n",
    "adjusted_county_cases_df = adjusted_county_cases_df.withColumn('lag', F.lag(adjusted_county_cases_df['covid_case_total'], 1).over(windowSpec))\n",
    "\n",
    "adjusted_county_cases_df = adjusted_county_cases_df.withColumn('covid_case_delta', \\\n",
    "    adjusted_county_cases_df['covid_case_total'] - adjusted_county_cases_df['lag'])\n",
    "\n",
    "adjusted_county_cases_df = adjusted_county_cases_df.drop('lag').drop('lead').drop('next_delta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_county_cases_df.agg({'covid_case_delta': 'min'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_county_cases_df.where(adjusted_county_cases_df['covid_case_delta'] < 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_county_cases_df.where(adjusted_county_cases_df['covid_case_delta'] < 0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_county_cases_df.where(adjusted_county_cases_df['covid_case_delta'] < -20).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_county_cases_df.where(adjusted_county_cases_df['covid_case_delta'] < -20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to have worked somewhat. The maximum drop is reduced, and we have less than one-fifth the amount of negative deltas as before, and only 12 outliers.  \n",
    "Spot checks on the source data shows that we eliminated the one-off drops, but there are still drops when the incorrect data persists over multiple days. I don't feel comfortable messing with the data this much since multi-day data entry issues seem unlikelier than that the case count did in fact change, due to correction, false positives, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do it all again for the death data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_deaths_df = load_covid_deaths_data(spark)\n",
    "covid_deaths_df.limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = [(pd.to_datetime(c) - unix_time) // second for c in covid_deaths_df.columns[6:]]\n",
    "\n",
    "time_data_columns = covid_deaths_df.columns[6:]\n",
    "time_data_columns.insert(0, 'fips')\n",
    "\n",
    "time_series = []\n",
    "\n",
    "def extract_county_death_data(row):\n",
    "    fips = time_data_columns[0]\n",
    "    for i in range(1, len(time_data_columns)):\n",
    "        time_series.append((row[fips], date_list[i - 1], row[time_data_columns[i]]))\n",
    "\n",
    "for row in covid_deaths_df.collect():\n",
    "    extract_county_death_data(row)\n",
    "\n",
    "time_series_columns = [\"fips\", \"timestamp\", \"covid_death_total\"]\n",
    "\n",
    "county_deaths_df = spark.createDataFrame(time_series, time_series_columns)\n",
    "\n",
    "windowSpec = Window \\\n",
    "    .partitionBy(county_deaths_df['fips']) \\\n",
    "    .orderBy(county_deaths_df['timestamp'].asc())\n",
    "\n",
    "county_deaths_df = county_deaths_df.withColumn('lag', F.lag(county_deaths_df['covid_death_total'], 1).over(windowSpec))\n",
    "county_deaths_df = county_deaths_df.withColumn('lead', F.lead(county_deaths_df['covid_death_total'], 1).over(windowSpec))\n",
    "\n",
    "# Populate deltas\n",
    "county_deaths_df = county_deaths_df.withColumn('covid_death_delta', \\\n",
    "    F.when(county_deaths_df['lag'].isNull(), 0) \\\n",
    "    .otherwise(county_deaths_df['covid_death_total'] - county_deaths_df['lag']))\n",
    "\n",
    "county_deaths_df = county_deaths_df.withColumn('next_delta', F.lead(county_deaths_df['covid_death_delta'], 1).over(windowSpec))\n",
    "\n",
    "# Fix overreporting\n",
    "county_deaths_df = county_deaths_df.withColumn('covid_death_total', \\\n",
    "    F.when((county_deaths_df['next_delta'] >= 0) | (county_deaths_df['lag'].isNull() | (county_deaths_df['lead'].isNull())), county_deaths_df['covid_death_total']) \\\n",
    "    .otherwise(F.ceil((county_deaths_df['lead'] + county_deaths_df['lag']) / 2)))\n",
    "\n",
    "# Recalculate deltas\n",
    "county_deaths_df = county_deaths_df.withColumn('lag', F.lag(county_deaths_df['covid_death_total'], 1).over(windowSpec))\n",
    "county_deaths_df = county_deaths_df.withColumn('covid_death_delta', \\\n",
    "    F.when(county_deaths_df['lag'].isNull(), 0) \\\n",
    "    .otherwise(county_deaths_df['covid_death_total'] - county_deaths_df['lag']))\n",
    "\n",
    "county_deaths_df = county_deaths_df.drop('lag').drop('lead').drop('next_delta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_deaths_df.agg({'covid_death_total': 'min'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_deaths_df.agg({'covid_death_total': 'max'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_deaths_df.where(county_deaths_df['covid_death_delta'].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_deaths_df.agg({'covid_death_delta': 'min'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_deaths_df.agg({'covid_death_delta': 'max'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_deaths_df.where(county_deaths_df['covid_death_delta'] < 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_deaths_df.where(county_deaths_df['covid_death_delta'] < -10).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_deaths_df.where(county_deaths_df['covid_death_delta'] < -10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Death data looks good, we do see some negative deltas again but the vast majority is very small, most likely due to corrections, overreporting, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_facts_df = county_cases_df.join(county_deaths_df, on=[\"fips\", \"timestamp\"], how=\"left\")\n",
    "county_facts_df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_facts_df.where(county_facts_df['fips'] == 1001).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, this seems to have worked! So now we have the Covid-19 case and death counts.\n",
    "\n",
    "Next up, do something similar for the weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tMin_df, tMax_df, cloud_df, wind_df = load_weather_data(spark)\n",
    "tMin_df.limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_weather_data(df, column_name):\n",
    "    date_list = [(pd.to_datetime(c) - unix_time) // second for c in df.columns[5:]]\n",
    "\n",
    "    time_data_columns = df.columns[5:]\n",
    "    time_data_columns.insert(0, 'fips')\n",
    "\n",
    "    time_series = []\n",
    "\n",
    "    def extract_weather_data(row):\n",
    "        fips = time_data_columns[0]\n",
    "        for i in range(1, len(time_data_columns)):\n",
    "            time_series.append((row[fips], date_list[i - 1], float(row[time_data_columns[i]])))\n",
    "\n",
    "    for row in df.collect():\n",
    "        extract_weather_data(row)\n",
    "\n",
    "    time_series_columns = [\"fips\", \"timestamp\", column_name]\n",
    "\n",
    "    df = spark.createDataFrame(time_series, time_series_columns)\n",
    "    \n",
    "    return df\n",
    "\n",
    "transformed_tMin_df = transform_weather_data(tMin_df, \"min_temp\")\n",
    "transformed_tMax_df = transform_weather_data(tMax_df, \"max_temp\")\n",
    "transformed_cloud_df = transform_weather_data(cloud_df, \"cloud_cover\")\n",
    "transformed_wind_df = transform_weather_data(wind_df, \"wind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_tMin_df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_facts_df = county_facts_df.join(transformed_tMin_df, on=[\"fips\", \"timestamp\"], how=\"left\")\n",
    "county_facts_df = county_facts_df.join(transformed_tMax_df, on=[\"fips\", \"timestamp\"], how=\"left\")\n",
    "county_facts_df = county_facts_df.join(transformed_cloud_df, on=[\"fips\", \"timestamp\"], how=\"left\")\n",
    "county_facts_df = county_facts_df.join(transformed_wind_df, on=[\"fips\", \"timestamp\"], how=\"left\")\n",
    "county_facts_df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_facts_df = county_facts_df.drop('date')\n",
    "county_facts_df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_facts_df.write.partitionBy('timestamp').mode('append').parquet(output_path + \"county_facts.parquet\")\n",
    "#county_facts_df.write.partitionBy('timestamp').mode('overwrite').parquet(output_path + \"county_facts.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
