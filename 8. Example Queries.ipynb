{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Example queries\n",
    "\n",
    "What use is a data engineering project without actually asking some questions about our data? Below are some example queries (run against the local parquet files since I don't want to pay for S3 read costs).\n",
    "\n",
    "##### Setup, imports and database loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup import create_spark_session\n",
    "\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from sql.exampleQueries import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started loading database\n",
      "Started loading time dimension table\n",
      "Finished loading time dimension table\n",
      "Started loading county dimension table\n",
      "Finished loading county dimension table\n",
      "Started loading state dimension table\n",
      "Finished loading state dimension table\n",
      "Started loading county facts table\n",
      "Finished loading county facts table\n",
      "Started loading state facts table\n",
      "Finished loading state facts table\n",
      "Finished loading database\n"
     ]
    }
   ],
   "source": [
    "from etl import load_all_tables\n",
    "\n",
    "time_dim_df, county_dim_df, state_dim_df, county_facts_df, state_facts_df = load_all_tables(spark)\n",
    "\n",
    "time_dim_df.createOrReplaceTempView(\"dim_time\")\n",
    "county_dim_df.createOrReplaceTempView(\"dim_county\")\n",
    "state_dim_df.createOrReplaceTempView(\"dim_state\")\n",
    "county_facts_df.createOrReplaceTempView(\"fact_county\")\n",
    "state_facts_df.createOrReplaceTempView(\"fact_state\")\n",
    "\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Queries\n",
    "Let's start simply. Which counties have the highest case rates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"\"\"\n",
    "    SELECT fc.fips, dc.county_name, max(fc.covid_case_total) as covid_case_total\n",
    "        FROM fact_county fc\n",
    "        LEFT JOIN dim_county dc\n",
    "        ON fc.fips == dc.fips\n",
    "        GROUP BY fc.fips, dc.county_name\n",
    "        ORDER BY covid_case_total DESC\n",
    "        LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which states have the most deaths in total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"\"\"\n",
    "    SELECT fs.state, max(fs.covid_case_total) as covid_case_total\n",
    "        FROM fact_state fs\n",
    "        GROUP BY fs.state\n",
    "        ORDER BY covid_case_total DESC\n",
    "        LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these are a bit misleading, though. Counties with higher population numbers are bound to have higher rates, so let's run these queries again but normalise them to the county/state population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"\"\"\n",
    "    WITH county_norm AS (\n",
    "        SELECT fc.fips, fc.covid_case_total / dc.population as norm_case_total\n",
    "        FROM fact_county fc\n",
    "        LEFT JOIN dim_county dc\n",
    "        ON fc.fips == dc.fips\n",
    "        GROUP BY fc.fips, fc.covid_case_total, dc.population\n",
    "    )\n",
    "    SELECT fc.fips, dc.county_name, dc.state, max(cn.norm_case_total) as norm_case_total\n",
    "        FROM fact_county fc\n",
    "        LEFT JOIN dim_county dc\n",
    "        ON fc.fips == dc.fips\n",
    "        LEFT JOIN county_norm cn\n",
    "        ON fc.fips == cn.fips\n",
    "        GROUP BY fc.fips, dc.county_name, dc.state\n",
    "        ORDER BY norm_case_total DESC\n",
    "        LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|        state|    norm_death_total|\n",
      "+-------------+--------------------+\n",
      "|   New Jersey| 0.00260851409661762|\n",
      "|     New York|0.002414721897611473|\n",
      "|Massachusetts|0.002326376900875365|\n",
      "| Rhode Island|0.002315298657448348|\n",
      "|  Mississippi|0.002233026288033...|\n",
      "|      Arizona|0.002226406601775...|\n",
      "| South Dakota|0.002137752412905...|\n",
      "|  Connecticut|0.002130622378532552|\n",
      "|    Louisiana|0.002057305849941...|\n",
      "|      Alabama|0.002031559343526046|\n",
      "+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"\"\"\n",
    "    WITH state_norm AS (\n",
    "        SELECT fs.state, fs.covid_death_total / ds.population as norm_death_total\n",
    "        FROM fact_state fs\n",
    "        LEFT JOIN dim_state ds\n",
    "        ON fs.state == ds.state\n",
    "        GROUP BY fs.state, fs.covid_death_total, ds.population\n",
    "    )\n",
    "    SELECT fs.state, max(sn.norm_death_total) as norm_death_total\n",
    "        FROM fact_state fs\n",
    "        LEFT JOIN state_norm sn\n",
    "        ON fs.state == sn.state\n",
    "        GROUP BY fs.state\n",
    "        ORDER BY norm_death_total DESC\n",
    "        LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for something a bit more tricky: Which counties have the largest temperature swings during the day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+----------+------------------+\n",
      "| fips|county_name|     state|       temp_change|\n",
      "+-----+-----------+----------+------------------+\n",
      "| 8031|     Denver|  Colorado|             32.97|\n",
      "| 8014| Broomfield|  Colorado|32.940000000000005|\n",
      "| 8001|      Adams|  Colorado|             32.75|\n",
      "| 8005|   Arapahoe|  Colorado|              32.4|\n",
      "| 8087|     Morgan|  Colorado|             32.25|\n",
      "|40025|   Cimarron|  Oklahoma|             32.19|\n",
      "|48431|   Sterling|     Texas|             32.04|\n",
      "| 8039|     Elbert|  Colorado|31.980000000000004|\n",
      "|56021|    Laramie|   Wyoming|             31.98|\n",
      "|35059|      Union|New Mexico|31.799999999999997|\n",
      "+-----+-----------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"\"\"\n",
    "    SELECT fc.fips, dc.county_name, dc.state, fc.max_temp - fc.min_temp as temp_change\n",
    "        FROM fact_county fc\n",
    "        LEFT JOIN dim_county dc\n",
    "        ON fc.fips == dc.fips\n",
    "        GROUP BY fc.fips, dc.county_name, dc.state, fc.max_temp, fc.min_temp\n",
    "        ORDER BY temp_change DESC\n",
    "        LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like Colorado is seeing quite a few temperature changes! It might be interesting to group this by state, or to narrow the query down a bit since it's now looking only at the most extreme temperature, not at the average deviation every day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd like to know how strongly Covid case rates correlate to population density. My guess is that high population density results in higher infection rates. To check this, we'll need to get the total case count for each county and normalise it for the total population, then compare the various percentiles of population density in the data set. We could split the whole dataset into 10% buckets of population density and calculate the average normalised case rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------+\n",
      "|bucket|average_normalised_cases|\n",
      "+------+------------------------+\n",
      "|   1.0|       34.96272359394068|\n",
      "|   0.9|        57.3299859120545|\n",
      "|   0.8|       53.69791338446918|\n",
      "|   0.7|        68.2151273536616|\n",
      "|   0.6|       55.55887627147644|\n",
      "|   0.5|       64.02359784609416|\n",
      "|   0.4|       67.70472271546825|\n",
      "|   0.3|       76.53304705111098|\n",
      "|   0.2|      118.82131288623005|\n",
      "|   0.1|      169.38397129450493|\n",
      "+------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"\"\"\n",
    "    WITH percentiles AS (\n",
    "        SELECT dc.fips, PERCENT_RANK() OVER(\n",
    "            ORDER BY dc.population_density ASC\n",
    "        ) AS percent_rank\n",
    "        FROM dim_county dc\n",
    "    ),\n",
    "    percent_buckets AS (\n",
    "        SELECT p.fips, ROUND(p.percent_rank, 1) AS bucket\n",
    "        FROM percentiles p\n",
    "    ),\n",
    "    max_cases AS (\n",
    "        SELECT fc.fips, max(fc.covid_case_total / dc.population_density) as normalised_covid_cases\n",
    "        FROM fact_county fc\n",
    "        LEFT JOIN dim_county dc\n",
    "        ON fc.fips == dc.fips\n",
    "        GROUP BY fc.fips\n",
    "    )\n",
    "    SELECT pb.bucket, avg(mc.normalised_covid_cases) as average_normalised_cases\n",
    "        FROM fact_county fc\n",
    "        LEFT JOIN percent_buckets pb\n",
    "        ON fc.fips == pb.fips\n",
    "        LEFT JOIN max_cases mc\n",
    "        ON fc.fips == mc.fips\n",
    "        GROUP BY pb.bucket\n",
    "        ORDER BY pb.bucket DESC\n",
    "        LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This didn't turn out how I expected, which is exciting! It looks like the top 10% most densely populated counties actually have a much lower case rate considering the overall population than the next 10%, then it stabilises throughout the middle and the bottom 10% are much, much worse off than the average.\n",
    "\n",
    "I'm not 100% confident that my query is correct, but this pretty much turned out exactly opposite what one would expect. I'd love to dig in more to evaluate factors that explain this result, maybe in a future project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm also curious about how strongly a county's average temperature correlates with case rates. I'd assume that warmer counties fare worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A harder question to answer is how much weather on any given day affects covid case rates a few days down the line.  \n",
    "Since the reporting won't be extremely accurate, we should limit ourselves to evaluating strings of consistently good/bad weather for a few days, and then look at the new case increase a week afterwards.\n",
    "\n",
    "First, we need to classify what makes a day \"good\" or \"bad\". I'd define a \"good\" day as having low chance of precipitation and low cloud cover, whereas a \"bad\" day is high on both. We could include temperature here as well, but we'd need to look at a rolling average to see how any given day compares; this presents a problem since rolling averages don't play well with the idea of having a period of good/bad days (they're subsequent days, so the last day in the sequence would need e.g. a higher temperature than the preceding days even though they were classed as \"good\" already).\n",
    "\n",
    "Next, we need to indentify sufficiently long sequences of days with similar weather.  \n",
    "Then we can determine the cases a week from each day, and track the delta.\n",
    "\n",
    "Finally, we can average the delta for good/bad weather days and see if we can find any difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "branch-env",
   "language": "python",
   "name": "branch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
