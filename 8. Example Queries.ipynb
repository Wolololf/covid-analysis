{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Example queries\n",
    "\n",
    "What use is a data engineering project without actually asking some questions about our data? Below are some example queries (run against the local parquet files since I don't want to pay for S3 read costs).\n",
    "\n",
    "##### Setup, imports and database loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup import create_spark_session\n",
    "\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from sql.exampleQueries import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started loading database\n",
      "Started loading time dimension table\n",
      "Finished loading time dimension table\n",
      "Started loading county dimension table\n",
      "Finished loading county dimension table\n",
      "Started loading state dimension table\n",
      "Finished loading state dimension table\n",
      "Started loading county facts table\n",
      "Finished loading county facts table\n",
      "Started loading state facts table\n",
      "Finished loading state facts table\n",
      "Finished loading database\n"
     ]
    }
   ],
   "source": [
    "from etl import load_all_tables\n",
    "\n",
    "time_dim_df, county_dim_df, state_dim_df, county_facts_df, state_facts_df = load_all_tables(spark)\n",
    "\n",
    "time_dim_df.createOrReplaceTempView(\"dim_time\")\n",
    "county_dim_df.createOrReplaceTempView(\"dim_county\")\n",
    "state_dim_df.createOrReplaceTempView(\"dim_state\")\n",
    "county_facts_df.createOrReplaceTempView(\"fact_county\")\n",
    "state_facts_df.createOrReplaceTempView(\"fact_state\")\n",
    "\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Queries\n",
    "Let's start simply. Which counties have the highest case rates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+-----------+----------------+\n",
      "| fips|    timestamp|county_name|covid_case_total|\n",
      "+-----+-------------+-----------+----------------+\n",
      "|13183|      Georgia|       Long|      1614384000|\n",
      "|29201|     Missouri|      Scott|      1614384000|\n",
      "|39129|         Ohio|   Pickaway|      1614384000|\n",
      "|48189|        Texas|       Hale|      1614384000|\n",
      "|40037|     Oklahoma|      Creek|      1614384000|\n",
      "|54069|West Virginia|       Ohio|      1614384000|\n",
      "|42041| Pennsylvania| Cumberland|      1614384000|\n",
      "|41039|       Oregon|       Lane|      1614384000|\n",
      "|48155|        Texas|      Foard|      1614384000|\n",
      "|17119|     Illinois|    Madison|      1614384000|\n",
      "+-----+-------------+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"\"\"\n",
    "    SELECT fc.fips, fc.timestamp, dc.county_name, max(fc.covid_case_total) as covid_case_total\n",
    "        FROM fact_county fc\n",
    "        LEFT JOIN dim_county dc\n",
    "        ON fc.fips == dc.fips\n",
    "        GROUP BY fc.fips, fc.timestamp, dc.county_name\n",
    "        ORDER BY covid_case_total DESC\n",
    "        LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|max(covid_case_total)|\n",
      "+---------------------+\n",
      "|           1614384000|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "county_facts_df.agg({'covid_case_total': 'max'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which states have the most deaths in total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"\"\"\n",
    "    SELECT fs.state, fs.timestamp, max(fs.covid_case_total) as covid_case_total\n",
    "        FROM fact_state fs\n",
    "        GROUP BY fs.state, fs.timestamp\n",
    "        ORDER BY covid_case_total DESC\n",
    "        LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for something a bit more tricky: Which counties have the largest temperature swings during the day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd like to know how strongly Covid case rates correlate to population density. My guess is that high population density results in higher infection rates. To calculate the correlation, we'll need to get the total case count for each county and normalise it for the total population, then make a model to fit those two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm also curious about how strongly a county's average temperature correlates with case rates. I'd assume that warmer counties fare worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A harder question to answer is how much weather on any given day affects covid case rates a few days down the line.  \n",
    "Since the reporting won't be extremely accurate, we should limit ourselves to evaluating strings of consistently good/bad weather for a few days, and then look at the new case increase a week afterwards.\n",
    "\n",
    "First, we need to classify what makes a day \"good\" or \"bad\". I'd define a \"good\" day as having low chance of precipitation and low cloud cover, whereas a \"bad\" day is high on both. We could include temperature here as well, but we'd need to look at a rolling average to see how any given day compares; this presents a problem since rolling averages don't play well with the idea of having a period of good/bad days (they're subsequent days, so the last day in the sequence would need e.g. a higher temperature than the preceding days even though they were classed as \"good\" already).\n",
    "\n",
    "Next, we need to indentify sufficiently long sequences of days with similar weather.  \n",
    "Then we can determine the cases a week from each day, and track the delta.\n",
    "\n",
    "Finally, we can average the delta for good/bad weather days and see if we can find any difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "branch-env",
   "language": "python",
   "name": "branch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
